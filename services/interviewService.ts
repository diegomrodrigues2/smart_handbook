import { GoogleGenAI } from "@google/genai";
import { InterviewQuestion, InterviewSession, InterviewMessage, InterviewEvaluation } from "../types";
import {
    getInterviewQuestionsPrompt,
    getInterviewEvaluationPrompt,
    getInterviewFollowUpPrompt,
    getInterviewFinalVerdictPrompt
} from "./prompts";

const apiKey = process.env.API_KEY || '';

let client: GoogleGenAI | null = null;

const getClient = () => {
    if (!client && apiKey) {
        client = new GoogleGenAI({ apiKey });
    }
    return client;
};

const MODEL_ID = "gemini-3-flash-preview";

// Research content from both research files for interview context
const INTERVIEW_RESEARCH_CONTENT = `
# Dom√≠nio Estrat√©gico de Entrevistas T√©cnicas Conceituais

## Framework CCMT (Conceito-Contexto-Mecanismo-Tradeoff)
- Conceito: Defina o termo com precis√£o usando terminologia padr√£o.
- Contexto: Explique o cen√°rio onde o conceito se torna relevante.
- Mecanismo: Descreva o processo interno. Use abordagem sequencial.
- Trade-off/Mitiga√ß√£o: Explique desvantagens ou como lidar com casos extremos.

## Matriz de Compara√ß√£o
Dimens√µes padr√£o para comparar sistemas:
- Estrutura de Dados/Modelo
- Modelo de Consist√™ncia (ACID vs BASE)
- Escalabilidade (Vertical vs Horizontal)
- Adequa√ß√£o ao Caso de Uso

## T√≥picos T√©cnicos Essenciais

### Internals de Banco de Dados
- B-Trees vs LSM Trees: trade-offs de leitura/escrita
- Teorema CAP e PACELC
- Sharding: Range-based, Hash-based, Consistent Hashing

### Concorr√™ncia
- Deadlocks e Condi√ß√µes de Coffman
- Thread vs Process: isolamento de mem√≥ria, troca de contexto
- GIL do Python, Goroutines do Go

### Sistemas Distribu√≠dos
- Consist√™ncia Eventual vs Forte
- Fan-out patterns
- CRDTs e Transforma√ß√£o Operacional

### Garbage Collection
- Mark-Sweep-Compact
- Gera√ß√µes: Young, Old, Metaspace
- G1 GC: regi√µes e pausas previs√≠veis

## Crit√©rios de Avalia√ß√£o
- Profundidade de Conhecimento vs Trivialidade
- An√°lise de Trade-offs (argumentar contra a pr√≥pria escolha)
- Clareza de Comunica√ß√£o
- Prospera na Ambiguidade

## N√≠veis de Sinal
- N√≠vel 1 (J√∫nior): Respostas superficiais, sem considerar escala
- N√≠vel 2 (Mid): Faz perguntas b√°sicas, precisa de dicas
- N√≠vel 3 (S√™nior): Define requisitos, calcula estimativas, justifica escolhas
- N√≠vel 4 (Staff): Antecipa requisitos futuros, desafia premissas, discute evolu√ß√£o
`;

import { arrayBufferToBase64 } from "./pdfContentService";

export const generateInterviewQuestions = async (
    noteContent: string,
    noteName: string,
    pdfData?: ArrayBuffer
): Promise<InterviewQuestion[]> => {
    const ai = getClient();
    if (!ai) {
        console.error("API client not available");
        return [];
    }

    try {
        const prompt = getInterviewQuestionsPrompt(pdfData ? "[PDF ATTACHED]" : noteContent, noteName);

        // Prepare content parts: PDF attachment if available, otherwise just text
        const contentParts: any[] = [];
        if (pdfData) {
            contentParts.push({
                inlineData: {
                    data: arrayBufferToBase64(pdfData),
                    mimeType: 'application/pdf'
                }
            });
            contentParts.push({ text: prompt });
        } else {
            contentParts.push({ text: prompt });
        }

        const response = await ai.models.generateContent({
            model: MODEL_ID,
            contents: [{ role: 'user', parts: contentParts }],
            config: {
                temperature: 0.7,
                responseMimeType: "application/json",
                responseSchema: {
                    type: "object",
                    properties: {
                        questions: {
                            type: "array",
                            items: {
                                type: "object",
                                properties: {
                                    id: { type: "string" },
                                    number: { type: "integer" },
                                    category: { type: "string" },
                                    difficulty: { type: "string" },
                                    question: { type: "string" },
                                    expectedTopics: { type: "array", items: { type: "string" } }
                                },
                                required: ["id", "number", "category", "difficulty", "question", "expectedTopics"]
                            }
                        }
                    },
                    required: ["questions"]
                }
            }
        });

        const text = response.text || "";
        const parsed = JSON.parse(text);
        return parsed.questions.map((q: any) => ({
            ...q,
            answered: false
        }));
    } catch (error) {
        console.error("Error generating interview questions:", error);
    }
    return [];
};

export const startInterviewSession = async (
    noteId: string,
    noteName: string,
    noteContent: string,
    pdfData?: ArrayBuffer
): Promise<InterviewSession | null> => {
    const questions = await generateInterviewQuestions(noteContent, noteName, pdfData);
    if (questions.length === 0) return null;

    return {
        noteId,
        noteName,
        questions,
        currentQuestionIndex: 0,
        messages: [],
        isComplete: false
    };
};

export const getInterviewerResponse = async (
    session: InterviewSession,
    noteContent: string,
    onChunk: (text: string) => void,
    recentImage?: { mimeType: string, data: string },
    pdfData?: ArrayBuffer
): Promise<void> => {
    const ai = getClient();
    if (!ai) {
        onChunk("Erro: Cliente de IA n√£o dispon√≠vel.");
        return;
    }

    const currentQuestion = session.questions[session.currentQuestionIndex];

    const dialogHistoryText = session.messages
        .map(m => `${m.role === 'interviewer' ? 'ENTREVISTADOR' : 'CANDIDATO'}: ${m.text}${m.imageUrl ? ' [Imagem Anexada]' : ''}`)
        .join('\n');

    const prompt = getInterviewFollowUpPrompt(
        currentQuestion,
        INTERVIEW_RESEARCH_CONTENT,
        pdfData ? "[PDF ATTACHED]" : noteContent,
        dialogHistoryText || 'In√≠cio da entrevista'
    );

    try {
        const parts: any[] = [{ text: prompt }];
        if (recentImage) {
            parts.push({
                inlineData: {
                    mimeType: recentImage.mimeType,
                    data: recentImage.data
                }
            });
        }
        if (pdfData) {
            parts.push({
                inlineData: {
                    data: arrayBufferToBase64(pdfData),
                    mimeType: 'application/pdf'
                }
            });
        }

        const responseStream = await ai.models.generateContentStream({
            model: MODEL_ID,
            contents: [{ role: 'user', parts }],
            config: { temperature: 0.7 }
        });

        for await (const chunk of responseStream) {
            if (chunk.text) {
                onChunk(chunk.text);
            }
        }
    } catch (error: any) {
        console.error("Error generating interviewer response:", error);
        onChunk(`[Erro: ${error.message || "Falha ao gerar resposta"}]`);
    }
};

export const evaluateCandidateResponse = async (
    session: InterviewSession,
    candidateResponse: string
): Promise<InterviewEvaluation | null> => {
    const ai = getClient();
    if (!ai) return null;

    const currentQuestion = session.questions[session.currentQuestionIndex];

    const dialogHistoryText = session.messages
        .map(m => `${m.role === 'interviewer' ? 'ENTREVISTADOR' : 'CANDIDATO'}: ${m.text}`)
        .join('\n');

    const prompt = getInterviewEvaluationPrompt(
        currentQuestion,
        candidateResponse,
        dialogHistoryText,
        INTERVIEW_RESEARCH_CONTENT
    );

    try {
        const response = await ai.models.generateContent({
            model: MODEL_ID,
            contents: prompt,
            config: {
                temperature: 0.5,
                topP: 0.9,
                responseMimeType: "application/json",
                responseSchema: {
                    type: "object",
                    properties: {
                        score: { type: "string", enum: ["strong_hire", "hire", "mixed", "no_hire"] },
                        dimensions: {
                            type: "object",
                            properties: {
                                depth: { type: "integer" },
                                tradeoffs: { type: "integer" },
                                communication: { type: "integer" }
                            }
                        },
                        feedback: { type: "string" },
                        strengths: { type: "array", items: { type: "string" } },
                        improvements: { type: "array", items: { type: "string" } }
                    },
                    required: ["score", "dimensions", "feedback", "strengths", "improvements"]
                }
            }
        });

        const text = response.text || "";
        return JSON.parse(text);
    } catch (error) {
        console.error("Error evaluating response:", error);
        return null;
    }
};

export const generateFinalVerdict = async (
    session: InterviewSession
): Promise<{ overallScore: 'strong_hire' | 'hire' | 'mixed' | 'no_hire'; summary: string; recommendation: string } | null> => {
    const ai = getClient();
    if (!ai) return null;

    const prompt = getInterviewFinalVerdictPrompt(session.questions, INTERVIEW_RESEARCH_CONTENT);

    try {
        const response = await ai.models.generateContent({
            model: MODEL_ID,
            contents: prompt,
            config: {
                temperature: 0.5,
                topP: 0.9,
                responseMimeType: "application/json",
                responseSchema: {
                    type: "object",
                    properties: {
                        overallScore: { type: "string", enum: ["strong_hire", "hire", "mixed", "no_hire"] },
                        summary: { type: "string" },
                        recommendation: { type: "string" }
                    },
                    required: ["overallScore", "summary", "recommendation"]
                }
            }
        });

        const text = response.text || "";
        return JSON.parse(text);
    } catch (error) {
        console.error("Error generating final verdict:", error);
        return null;
    }
};

export const generateModelAnswer = async (
    question: InterviewQuestion,
    noteContent: string,
    onChunk: (text: string) => void,
    pdfData?: ArrayBuffer
): Promise<void> => {
    const ai = getClient();
    if (!ai) {
        onChunk("Erro: Cliente de IA n√£o dispon√≠vel.");
        return;
    }

    const categoryLabels: Record<string, string> = {
        'database_internals': 'Internals de Banco de Dados',
        'concurrency': 'Concorr√™ncia e Multithreading',
        'distributed_systems': 'Sistemas Distribu√≠dos',
        'networking': 'Redes e Protocolos',
        'languages_runtimes': 'Linguagens e Runtimes',
        'os_fundamentals': 'Fundamentos de Sistemas Operacionais'
    };

    const prompt = `Voc√™ √© um engenheiro s√™nior experiente em entrevistas t√©cnicas.

CONTEXTO DA NOTA DE ESTUDO:
${pdfData ? "[PDF ANEXADO]" : noteContent}

METODOLOGIA DE ENTREVISTAS T√âCNICAS:
${INTERVIEW_RESEARCH_CONTENT}

QUEST√ÉO A SER RESPONDIDA:
Categoria: ${categoryLabels[question.category] || question.category}
Dificuldade: ${question.difficulty}
Pergunta: ${question.question}
T√≥picos Esperados: ${question.expectedTopics.join(', ')}

TAREFA:
Gere uma RESPOSTA MODELO exemplar que demonstra como um candidato de n√≠vel Staff+ deveria responder a esta quest√£o.

A resposta deve seguir o framework CCMT:
1. **Conceito**: Defini√ß√£o precisa usando terminologia padr√£o
2. **Contexto**: Cen√°rio onde o conceito se torna relevante
3. **Mecanismo**: Descri√ß√£o do processo interno, passo a passo
4. **Trade-off**: An√°lise de vantagens/desvantagens e quando usar ou n√£o

FORMATO DA RESPOSTA:
- Use Markdown formatado
- Seja abrangente mas conciso
- Inclua exemplos pr√°ticos quando relevante
- Mencione trade-offs e alternativas
- Demonstre profundidade t√©cnica sem ser excessivamente verboso

Responda em portugu√™s brasileiro.`;

    try {
        const parts: any[] = [{ text: prompt }];
        if (pdfData) {
            parts.push({
                inlineData: {
                    data: arrayBufferToBase64(pdfData),
                    mimeType: 'application/pdf'
                }
            });
        }

        const responseStream = await ai.models.generateContentStream({
            model: MODEL_ID,
            contents: [{ role: 'user', parts }],
            config: { temperature: 0.6 }
        });

        for await (const chunk of responseStream) {
            if (chunk.text) {
                onChunk(chunk.text);
            }
        }
    } catch (error: any) {
        console.error("Error generating model answer:", error);
        onChunk(`[Erro: ${error.message || "Falha ao gerar resposta modelo"}]`);
    }
};

export const generateInterviewTranscript = async (
    session: InterviewSession,
    noteName: string
): Promise<string> => {
    const scoreLabels: Record<string, string> = {
        'strong_hire': 'üü¢ FORTE CONTRATA√á√ÉO',
        'hire': 'üü° CONTRATA√á√ÉO',
        'mixed': 'üü† MISTO/TALVEZ',
        'no_hire': 'üî¥ N√ÉO CONTRATAR'
    };

    const categoryLabels: Record<string, string> = {
        'database_internals': 'Internals de Banco de Dados',
        'concurrency': 'Concorr√™ncia e Multithreading',
        'distributed_systems': 'Sistemas Distribu√≠dos',
        'networking': 'Redes e Protocolos',
        'languages_runtimes': 'Linguagens e Runtimes',
        'os_fundamentals': 'Fundamentos de Sistemas Operacionais'
    };

    let content = `# Transcri√ß√£o de Entrevista T√©cnica\n\n`;
    content += `**Tema Base:** ${noteName}\n`;
    content += `**Data:** ${new Date().toLocaleString('pt-BR')}\n`;
    content += `**Total de Quest√µes:** ${session.questions.length}\n\n`;

    // Final Verdict Summary
    if (session.finalVerdict) {
        content += `---\n\n## üìä Veredicto Final\n\n`;
        content += `**Resultado:** ${scoreLabels[session.finalVerdict.overallScore]}\n\n`;
        content += `### Resumo\n${session.finalVerdict.summary}\n\n`;
        content += `### Recomenda√ß√£o\n${session.finalVerdict.recommendation}\n\n`;
    }

    content += `---\n\n## üìù Quest√µes e Avalia√ß√µes\n\n`;

    for (const question of session.questions) {
        content += `### Quest√£o ${question.number}: ${categoryLabels[question.category] || question.category}\n\n`;
        content += `**Dificuldade:** ${question.difficulty.toUpperCase()}\n\n`;
        content += `**Pergunta:**\n> ${question.question}\n\n`;

        // Get all messages for this question
        const questionMessages = session.messages.filter(m => m.questionId === question.id);

        // Look for model answer message (starts with "## üìö Resposta Modelo")
        const modelAnswerMsg = questionMessages.find(m =>
            m.role === 'interviewer' && m.text.includes('## üìö Resposta Modelo')
        );

        if (modelAnswerMsg) {
            // Extract the model answer (remove the header)
            const modelAnswerContent = modelAnswerMsg.text.replace('## üìö Resposta Modelo\n\n', '');
            content += `**üìö Resposta Modelo:**\n\n${modelAnswerContent}\n\n`;
        } else if (question.candidateResponse && question.candidateResponse !== '[Resposta modelo gerada automaticamente]') {
            // Include actual candidate response if it exists
            content += `**Resposta do Candidato:**\n${question.candidateResponse}\n\n`;

            // Also include the dialogue if there were follow-up exchanges
            const candidateMessages = questionMessages.filter(m => m.role === 'candidate');
            const interviewerResponses = questionMessages.filter(m =>
                m.role === 'interviewer' && !m.text.includes('## üìö Resposta Modelo')
            ).slice(1); // Skip the first interviewer message (question presentation)

            if (candidateMessages.length > 0 || interviewerResponses.length > 0) {
                content += `**Di√°logo Completo:**\n\n`;
                for (const msg of questionMessages) {
                    if (msg.role === 'interviewer' && !msg.text.includes('## üìö Resposta Modelo')) {
                        content += `**üéôÔ∏è Entrevistador:**\n${msg.text}\n\n`;
                    } else if (msg.role === 'candidate') {
                        content += `**üí¨ Candidato:**\n${msg.text}\n\n`;
                    }
                }
            }
        } else if (question.candidateResponse) {
            content += `**Resposta do Candidato:**\n${question.candidateResponse}\n\n`;
        }

        if (question.evaluation) {
            const eval_ = question.evaluation;
            content += `**Avalia√ß√£o:** ${scoreLabels[eval_.score]}\n\n`;
            content += `| Dimens√£o | Nota (1-4) |\n`;
            content += `|----------|------------|\n`;
            content += `| Profundidade de Conhecimento | ${eval_.dimensions.depth}/4 |\n`;
            content += `| An√°lise de Trade-offs | ${eval_.dimensions.tradeoffs}/4 |\n`;
            content += `| Clareza de Comunica√ß√£o | ${eval_.dimensions.communication}/4 |\n\n`;

            content += `**Feedback:**\n${eval_.feedback}\n\n`;

            if (eval_.strengths.length > 0) {
                content += `**Pontos Fortes:**\n`;
                eval_.strengths.forEach(s => content += `- ‚úÖ ${s}\n`);
                content += `\n`;
            }

            if (eval_.improvements.length > 0) {
                content += `**√Åreas de Melhoria:**\n`;
                eval_.improvements.forEach(i => content += `- üìå ${i}\n`);
                content += `\n`;
            }
        }

        content += `---\n\n`;
    }

    content += `\n*Gerado automaticamente pelo Smart Handbook Interview Mode em ${new Date().toLocaleString('pt-BR')}*\n`;

    return content;
};
